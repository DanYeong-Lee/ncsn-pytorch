{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:01<00:00,  6.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from src.model import RefineNet\n",
    "from src.score_matching import linear_noise_scale\n",
    "from src.langevin_dynamics import sample\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "noise_steps = 10\n",
    "\n",
    "model = RefineNet(\n",
    "    in_channels=1,\n",
    "    hidden_channels=(128, 256, 512, 1024),\n",
    "    n_noise_scale=noise_steps\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/refinenet_mnist.pth'))\n",
    "noise_scales = linear_noise_scale(start=1., end=0.01, length=10).to(device)\n",
    "samples = sample(model, (8, 1, 28, 28), noise_scales, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACS0lEQVR4nE3BzWucVRjG4d95zvOcc973zcxkrCG21o82JlRKk1K1NVZQlKBQVyKIKxVXCiLoQigu9D9QxK1LVyq4EBQKuqwrNyIVvzZCQRBqZYjpZOZ263VRUrRCqR1D1IRtEtZRWc3VwfBUNroYSAxpoNBjjAkmFLLnjNc0pDPheYx7Y4hoD03AaTjHyWGb1Z8CehodRqWVjjGV+16saSXMorvHnWmx6h1eMCr5ZMcGmZy+kdYKRk9Qk2M2BT46eJ7wO3+Q9qyndwwgY4z4TNJlKtL+XgDRbqOL1nLJNtmXdAIuzqQhRcpRsP0jt+YL7uZGg0//xJ7uYWb4YnnkXp44223BYz9Len+c8uQP6WwdP/ua4RWMin8h6Z0pj/DuQtfLM9el7aCQrLZ4RdIbrX+QDyV98qgk/ZVTBSenC5Kkmx/0VyTNJOn314NCid5iS5KkA+mmJGn2VkT0A1SCzYtv6n9+vERgdHQUyksvcPq5HUnStcur3cnc1Y5aoJHvuvDkZO/Y2suSNG3gMRQCqDZ46kaMY1eSXjV7OJtnhrTS/CDPyvbqr7f+mdwPvPfVkqvt4OixGw98Pl8yYSv1NM6fkfRdsZZSijR4tWoQNAK742tJb5MI1vHdLSJlK4dTtUWzU7fD7OOQL319d/23clq5gGPJKdckPe6UdA4jjzomKaxaZD/s9SVc+TbXRf3JpqO20/niROKU0+UGbNIoEBlS5ErZLqnb4Jc0T8pL2SHt6Nr3c5DPy/Tvf7Hj+E4iqMBwLrDUE6xYYfQfb2jFNcWg8IcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = torch.clamp(samples, 0., 1.)\n",
    "to_pil_image(samples[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch20",
   "language": "python",
   "name": "pytorch20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
